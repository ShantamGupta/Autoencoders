{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For  20NG TF as an autoencoder with a desired hidden layer size (try K=20, 100, 200). Verify the obtained reencoding of data (the new feature representation) in several ways: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import mnist \n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.104553     0.213015     0.280656     0.065425     0.312223   \n",
       "std       0.305358     1.290575     0.504143     1.395151     0.672513   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.420000     0.000000     0.380000   \n",
       "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.095901     0.114208     0.105295     0.090067     0.239413   \n",
       "std       0.273824     0.391441     0.401071     0.278616     0.644755   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.160000   \n",
       "max       5.880000     7.270000    11.110000     5.260000    18.180000   \n",
       "\n",
       "          ...                48           49           50           51  \\\n",
       "count     ...       4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      ...          0.038575     0.139030     0.016976     0.269071   \n",
       "std       ...          0.243471     0.270355     0.109394     0.815672   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.065000     0.000000     0.000000   \n",
       "75%       ...          0.000000     0.188000     0.000000     0.315000   \n",
       "max       ...          4.385000     9.752000     4.081000    32.478000   \n",
       "\n",
       "                52           53           54           55            56  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   4601.000000   \n",
       "mean      0.075811     0.044238     5.191515    52.172789    283.289285   \n",
       "std       0.245882     0.429342    31.729449   194.891310    606.347851   \n",
       "min       0.000000     0.000000     1.000000     1.000000      1.000000   \n",
       "25%       0.000000     0.000000     1.588000     6.000000     35.000000   \n",
       "50%       0.000000     0.000000     2.276000    15.000000     95.000000   \n",
       "75%       0.052000     0.000000     3.706000    43.000000    266.000000   \n",
       "max       6.003000    19.829000  1102.500000  9989.000000  15841.000000   \n",
       "\n",
       "                57  \n",
       "count  4601.000000  \n",
       "mean      0.394045  \n",
       "std       0.488698  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training data \n",
    "\n",
    "path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "\n",
    "data = pd.read_csv(path, header = None)\n",
    "\n",
    "#check out the summary \n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting 20% data into test and 80 % into train \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data.iloc[:,0:57], data.iloc[:,57], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3680, 57), (3680,), (921, 57), (921,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulding Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy using Decision Trees: 0.9994565217391305\n",
      "Test Accuracy using Decision Trees: 0.9120521172638436\n"
     ]
    }
   ],
   "source": [
    "#Building the Decision Tree Model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "#Accuracy on test data\n",
    "print(\"Train Accuracy using Decision Trees:\",clf.score(X_train,Y_train))\n",
    "\n",
    "#Accuracy on test data\n",
    "print(\"Test Accuracy using Decision Trees:\",clf.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input & Output Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = 57\n",
    "hidden_layer1_units = 20\n",
    "output_layer_units = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, inputs]) #flattened shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation\n",
    "$\\Pr{(Y=j | X)}  = \\frac{e^{(X.W_j^T + B_j)}}{\\sum\\limits_{i = 1}^n {e^{(X.W_i^T + B_i)}}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)\n",
    "W = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([inputs, hidden_layer1_units])),\n",
    "    'decoder_h1':tf.Variable(tf.random_normal([hidden_layer1_units, output_layer_units]))\n",
    "}\n",
    "\n",
    "B = {\n",
    "    'encoder_h1':tf.Variable(tf.random_normal([hidden_layer1_units])),\n",
    "    'decoder_h1':tf.Variable(tf.random_normal([output_layer_units]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = tf.matmul(X,W['encoder_h1']) +  B['encoder_h1']\n",
    "encoder = tf.sigmoid(encoder) # using softmax activation\n",
    "decoder = tf.matmul(encoder,W['decoder_h1'])  + B['decoder_h1']\n",
    "decoder = tf.nn.softmax(decoder) # using softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function: Cross Entropy\n",
    "## Optimizer : Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = X\n",
    "loss = tf.reduce_mean(tf.pow(decoder - Y, 2))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(batch_size,x):\n",
    "        indexes = list(range(x.shape[0]))\n",
    "        random.shuffle(indexes)\n",
    "        ind = indexes[:batch_size]\n",
    "        return(x[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Average Train Eror: 6208.04390625\n",
      "\n",
      " Test Error 11645.852\n",
      "Epoch: 6 \t Average Train Eror: 5480.756123046875\n",
      "\n",
      " Test Error 11645.351\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        train_c = 0                                                       #cost\n",
    "        total_batch = int(X_train.shape[0]/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x = get_batches(batch_size, np.array(X_train))\n",
    "            _,train_c = sess.run([optimizer, loss], feed_dict = {X: batch_x})\n",
    "            train_c += train_c/batch_size    \n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\",epoch+1,\"\\t Average Train Eror:\",train_c)  \n",
    "            test_c = sess.run(loss,feed_dict = {X:np.array(X_test)})\n",
    "            print(\"\\n Test Error\", test_c)    \n",
    "    encoder_train, decoder_train = sess.run([encoder,decoder],feed_dict = {X:np.array(X_train)})\n",
    "    encoder_test, decoder_test = sess.run([encoder,decoder],feed_dict = {X:np.array(X_test)})       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3680, 20), (3680, 57), (921, 20), (921, 57))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " encoder_train.shape, decoder_train.shape, encoder_test.shape, decoder_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model with reduced dimensions k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy using Decision Trees: 0.8195652173913044\n",
      "Test Accuracy using Decision Trees: 0.7014115092290988\n"
     ]
    }
   ],
   "source": [
    "#Building the Decision Tree Model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(encoder_train,Y_train)\n",
    "\n",
    "#Accuracy on test data\n",
    "print(\"Train Accuracy using Decision Trees:\",clf.score(encoder_train,Y_train))\n",
    "\n",
    "#Accuracy on test data\n",
    "print(\"Test Accuracy using Decision Trees:\",clf.score(encoder_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Average Train Eror: 2875.4818359375\n",
      "\n",
      " Test Error 11654.148\n",
      "Epoch: 6 \t Average Train Eror: 4919.331743164063\n",
      "\n",
      " Test Error 11645.346\n"
     ]
    }
   ],
   "source": [
    "inputs = 57\n",
    "hidden_layer1_units = 30\n",
    "output_layer_units = inputs\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, inputs]) #flattened shape\n",
    "\n",
    "tf.set_random_seed(42)\n",
    "W = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([inputs, hidden_layer1_units])),\n",
    "    'decoder_h1':tf.Variable(tf.random_normal([hidden_layer1_units, output_layer_units]))\n",
    "}\n",
    "\n",
    "B = {\n",
    "    'encoder_h1':tf.Variable(tf.random_normal([hidden_layer1_units])),\n",
    "    'decoder_h1':tf.Variable(tf.random_normal([output_layer_units]))\n",
    "}\n",
    "\n",
    "\n",
    "encoder = tf.matmul(X,W['encoder_h1']) +  B['encoder_h1']\n",
    "encoder = tf.sigmoid(encoder) # using softmax activation\n",
    "decoder = tf.matmul(encoder,W['decoder_h1'])  + B['decoder_h1']\n",
    "decoder = tf.nn.softmax(decoder) # using softmax activation\n",
    "\n",
    "Y = X\n",
    "loss = tf.reduce_mean(tf.pow(decoder - Y, 2))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "def get_batches(batch_size,x):\n",
    "        indexes = list(range(x.shape[0]))\n",
    "        random.shuffle(indexes)\n",
    "        ind = indexes[:batch_size]\n",
    "        return(x[ind])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        train_c = 0                                                       #cost\n",
    "        total_batch = int(X_train.shape[0]/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x = get_batches(batch_size, np.array(X_train))\n",
    "            _,train_c = sess.run([optimizer, loss], feed_dict = {X: batch_x})\n",
    "            train_c += train_c/batch_size    \n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\",epoch+1,\"\\t Average Train Eror:\",train_c)  \n",
    "            test_c = sess.run(loss,feed_dict = {X:np.array(X_test)})\n",
    "            print(\"\\n Test Error\", test_c)    \n",
    "    encoder_train_30, decoder_train_30 = sess.run([encoder,decoder],feed_dict = {X:np.array(X_train)})\n",
    "    encoder_test_30, decoder_test_30 = sess.run([encoder,decoder],feed_dict = {X:np.array(X_test)})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3680, 30), (3680, 57), (921, 30), (921, 57))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_train_30.shape, decoder_train_30.shape,encoder_test_30.shape, decoder_test_30.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model with reduced dimensions k = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy using Decision Trees: 0.9695652173913043\n",
      "Test Accuracy using Decision Trees: 0.7654723127035831\n"
     ]
    }
   ],
   "source": [
    "#Building the Decision Tree Model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(encoder_train_30,Y_train)\n",
    "\n",
    "#Accuracy on test data\n",
    "print(\"Train Accuracy using Decision Trees:\",clf.score(encoder_train_30,Y_train))\n",
    "\n",
    "#Accuracy on test data\n",
    "print(\"Test Accuracy using Decision Trees:\",clf.score(encoder_test_30,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K = 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Average Train Eror: 19716.76826171875\n",
      "\n",
      " Test Error 11654.139\n",
      "Epoch: 6 \t Average Train Eror: 32150.761875\n",
      "\n",
      " Test Error 11645.333\n"
     ]
    }
   ],
   "source": [
    "inputs = 57\n",
    "hidden_layer1_units = 40\n",
    "output_layer_units = inputs\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, inputs]) #flattened shape\n",
    "\n",
    "tf.set_random_seed(42)\n",
    "W = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([inputs, hidden_layer1_units])),\n",
    "    'decoder_h1':tf.Variable(tf.random_normal([hidden_layer1_units, output_layer_units]))\n",
    "}\n",
    "\n",
    "B = {\n",
    "    'encoder_h1':tf.Variable(tf.random_normal([hidden_layer1_units])),\n",
    "    'decoder_h1':tf.Variable(tf.random_normal([output_layer_units]))\n",
    "}\n",
    "\n",
    "\n",
    "encoder = tf.matmul(X,W['encoder_h1']) +  B['encoder_h1']\n",
    "encoder = tf.sigmoid(encoder) # using softmax activation\n",
    "decoder = tf.matmul(encoder,W['decoder_h1'])  + B['decoder_h1']\n",
    "decoder = tf.nn.softmax(decoder) # using softmax activation\n",
    "\n",
    "Y = X\n",
    "loss = tf.reduce_mean(tf.pow(decoder - Y, 2))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "def get_batches(batch_size,x):\n",
    "        indexes = list(range(x.shape[0]))\n",
    "        random.shuffle(indexes)\n",
    "        ind = indexes[:batch_size]\n",
    "        return(x[ind])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        train_c = 0                                                       #cost\n",
    "        total_batch = int(X_train.shape[0]/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x = get_batches(batch_size, np.array(X_train))\n",
    "            _,train_c = sess.run([optimizer, loss], feed_dict = {X: batch_x})\n",
    "            train_c += train_c/batch_size    \n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\",epoch+1,\"\\t Average Train Eror:\",train_c)  \n",
    "            test_c = sess.run(loss,feed_dict = {X:np.array(X_test)})\n",
    "            print(\"\\n Test Error\", test_c)    \n",
    "    encoder_train_40, decoder_train_40 = sess.run([encoder,decoder],feed_dict = {X:np.array(X_train)})\n",
    "    encoder_test_40, decoder_test_40 = sess.run([encoder,decoder],feed_dict = {X:np.array(X_test)})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3680, 40), (3680, 57))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_train_40.shape, decoder_train_40.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model with reduced dimensions k = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy using Decision Trees: 0.9741847826086957\n",
      "Test Accuracy using Decision Trees: 0.7785016286644951\n"
     ]
    }
   ],
   "source": [
    "#Building the Decision Tree Model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(encoder_train_40,Y_train)\n",
    "\n",
    "#Accuracy on test data\n",
    "print(\"Train Accuracy using Decision Trees:\",clf.score(encoder_train_40,Y_train))\n",
    "\n",
    "#Accuracy on test data\n",
    "print(\"Test Accuracy using Decision Trees:\",clf.score(encoder_test_40,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
